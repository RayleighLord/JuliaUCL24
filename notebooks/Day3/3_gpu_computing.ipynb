{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"./imgs/Julia-code-cpu-gpu.png\" width=768>\n",
    "\n",
    "* **host**: CPU + system memory (host memory)\n",
    "* **device**: GPU with its memory (device memory) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NVIDIA A100 GPU\n",
    "\n",
    "<img src=\"./imgs/a100_front.png\" width=512px>\n",
    "\n",
    "**Source:** [NVIDIA whitepaper](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Streaming Multiprocessor in NVIDIA A100**\n",
    "\n",
    "<img src=\"./imgs/a100_SM.png\" width=512px>\n",
    "\n",
    "**Source:** [NVIDIA whitepaper](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)\n",
    "\n",
    "| Compute units              | Count            |\n",
    "|----------------------------|------------------|\n",
    "| **SMs**                    | 108              |\n",
    "| **CUDA cores** / FP32 ALUs | 6912 (64 per SM) |\n",
    "| **Tensor cores**           | 432 (4 per SM)   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/gpu_topology.svg\" width=500px>\n",
    "\n",
    "* **host**: CPU + system memory (host memory)\n",
    "* **device**: GPU with its memory (device memory)\n",
    "* **SM**: Streaming Multiprocessor\n",
    "\n",
    "Communication:\n",
    "* Host-device bandwidth: **31.5 GB/s**\n",
    "* GPU global memory bandwidth: **1555 GB/s**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick comparison: CPU vs GPU\n",
    "\n",
    "### AMD EPYC 7763 vs NVIDIA A100\n",
    "\n",
    "|               | compute units   | maximum clock frequency [GHz] | FP32 peak performance [TFLOPS] |\n",
    "|:-------------:|:---------------:|:-----------------------------:|:------------------------------:|\n",
    "| AMD EPYC 7763 |  64 x86 cores   |  3.50                         |  5.0                           |\n",
    "| NVIDIA A100   | 6912 CUDA cores |  1.41                         | 19.5 (155.9 for Tensor cores)                     |\n",
    "\n",
    "Most of the [top500](https://top500.org/lists/top500/2023/11/) systems have GPUs as accelerators and they are dominating!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"./imgs/cpu_gpu_fraction.svg\" width=768>\n",
    "\n",
    "**Source:** [J. Apostolakis et al., *Detector simulation challenges for future accelerator experiments.*, Frontiers in Physics 10 (2022)](https://doi.org/10.3389/fphy.2022.913510)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences between CPU and GPU\n",
    "\n",
    "|                   | CPU                               | GPU                                 |\n",
    "|:-----------------:|:---------------------------------:|:-----------------------------------:|\n",
    "| optimized for     | latency and per-core performance  | computational throughput            |\n",
    "| cores             | complex                           | rather simple                       |\n",
    "| number of threads | O(100)                            | O(1_000_000)                        |\n",
    "| thread pinning    | a must for good performance       | not needed at all                   |\n",
    "\n",
    "### Memory-bound scientific computing\n",
    "\n",
    "The performance of most scientific codes is **memory-bound** (memory access speed) rather than compute-bound (how fast computations can be done). In a certain time interval, GPUs (and CPUs) can perform more computations than read numbers from memory.\n",
    "\n",
    "**Peak performance over peak memory bandwidth** (for NVIDIA A100 40GB SXM)\n",
    "\n",
    "$$\n",
    "\\dfrac{19.5 \\ [\\textrm{TFLOPS}]}{1.56 \\ [\\textrm{TB/s}]} \\cdot 4 \\ \\textrm{B} = 50\\ \\textrm{FLOPs}\n",
    "$$\n",
    "\n",
    "To achieve the peak FP32 performance one needs 50 FLOPs per FP32 number (i.e. `Float32`) from memory.\n",
    "\n",
    "Crucially, the peak memory bandwidth of GPUs is much higher than for CPUs: **~1.56 TB/s** (A100 40GB SXM) vs **~400 GB/s** (2x AMD EPYC 7763).\n",
    "\n",
    "(‚Üí exercise **saxpy_gpu** and **daxpy_cpu**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Julia + GPU (NVIDIA)\n",
    "\n",
    "Website: https://juliagpu.org/\n",
    "\n",
    "We'll focus on **NVIDIA GPUs** but there is [support for other GPUs](https://juliagpu.org/) (AMD, Intel, etc.) as well.\n",
    "\n",
    "The interface to NVIDIA GPU computing is the [CUDA language extension](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html). In Julia there is [CUDA.jl](https://github.com/JuliaGPU/CUDA.jl).\n",
    "\n",
    "It leverages LLVM, specifically parts of the Julia compiler as well as [GPUCompiler.jl](https://github.com/JuliaGPU/GPUCompiler.jl), to compile **native GPU code**. (compare to `nvcc`)\n",
    "\n",
    "It provides:\n",
    "\n",
    "* **High-level abstraction `CuArray`**\n",
    "* **Tools for writing CUDA kernels**\n",
    "* **Wrappers to proprietary NVIDIA libraries (e.g. cuBLAS, cuFFT, cuSOLVER, cuSPARSE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting CUDA\n",
    "\n",
    "By default, **it's automatic**. The CUDA toolkit is installed automatically when **using** CUDA.jl for the first time. The only requirement is a working NVIDIA driver.\n",
    "\n",
    "**Note:** You can readily add CUDA.jl to a Julia environment on a machine without GPUs, say, a login node. See [Precompiling CUDA.jl without CUDA](https://cuda.juliagpu.org/stable/installation/overview/#Precompiling-CUDA.jl-without-CUDA) for more information.\n",
    "\n",
    "#### System CUDA\n",
    "\n",
    "You can opt-out of the automatic system by setting a Julia preference, e.g.\n",
    "\n",
    "```julia\n",
    "CUDA.set_runtime_version!(v\"12.2\"; local_toolkit=true)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA runtime 12.2, artifact installation\n",
      "CUDA driver 12.2\n",
      "NVIDIA driver 535.154.5\n",
      "\n",
      "CUDA libraries: \n",
      "- CUBLAS: 12.2.5\n",
      "- CURAND: 10.3.3\n",
      "- CUFFT: 11.0.8\n",
      "- CUSOLVER: 11.5.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- CUSPARSE: 12.1.2\n",
      "- CUPTI: 20.0.0\n",
      "- NVML: 12.0.0+535.154.5\n",
      "\n",
      "Julia packages: \n",
      "- CUDA: 5.1.2\n",
      "- CUDA_Driver_jll: 0.7.0+1\n",
      "- CUDA_Runtime_jll: 0.10.1+0\n",
      "\n",
      "Toolchain:\n",
      "- Julia: 1.10.0\n",
      "- LLVM: 15.0.7\n",
      "\n",
      "Preferences:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- CUDA_Runtime_jll.version: 12.2\n",
      "\n",
      "1 device:\n",
      "  0: NVIDIA GeForce RTX 3070 Ti Laptop GPU (sm_86, 7.372 GiB / 8.000 GiB available)\n"
     ]
    }
   ],
   "source": [
    "CUDA.versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.functional() # if this works, you're good to go üëç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuDevice(0): NVIDIA GeForce RTX 3070 Ti Laptop GPU"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device() # the currently selected GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array programming: `CuArray`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to use a GPU is via **vectorized array operations** (e.g. broadcasting). Each of these operations will be backed by one or more GPU kernels, either natively written in Julia or from some application library. As long as your data is large enough you should be able to get nice speedups in many cases.\n",
    "\n",
    "You use the `CuArray` type, which serves a dual purpose:\n",
    "\n",
    "* a managed container for GPU memory\n",
    "* a way to dispatch to operations that execute on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `CuArray` is a **CPU object representing GPU memory**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_gpu = CuArray{Float32}(undef, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.7391175\n",
       " 0.84908134\n",
       " 0.046983227\n",
       " 0.64861226"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.rand(4) # Note: defaults to Float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.zeros(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can readily move data to the GPU by converting to `CuArray`.\n",
    "\n",
    " <img src=\"./imgs/cpu_gpu_transfer.svg\" width=180px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element CuArray{Int64, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_cpu = [1,2,3,4]\n",
    "x_gpu = CuArray(x_cpu) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(or by using `copyto!` or `copy!` to move it into already allocated memory)\n",
    "\n",
    "For better performance the data movement between CPU and GPU should be minimized as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array computations on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CuArray <: AbstractArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we should be able to do all kind of operations with it, that we'd also do with regular `Array`s. (**duck typing**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = 2048\n",
    "A_gpu = CUDA.rand(N,N);\n",
    "B_gpu = CUDA.rand(N,N);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048√ó2048 CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 501.325  520.195  512.339  508.041  ‚Ä¶  506.885  515.095  506.574  511.452\n",
       " 490.967  495.588  507.527  491.685     497.625  506.091  488.61   502.771\n",
       " 502.253  510.948  513.907  501.452     505.972  513.187  503.681  513.226\n",
       " 496.215  510.388  509.005  506.202     507.746  509.221  497.762  507.773\n",
       " 499.031  506.784  514.251  507.418     511.988  511.682  493.963  507.552\n",
       " 502.38   514.254  529.072  511.22   ‚Ä¶  517.469  522.785  510.055  521.209\n",
       " 493.301  506.185  512.416  503.791     502.829  505.601  491.318  505.199\n",
       " 505.15   517.611  509.744  517.33      516.554  524.496  509.856  516.612\n",
       " 504.27   519.443  523.288  508.838     513.612  517.515  511.861  513.502\n",
       " 502.099  502.83   511.259  499.184     506.761  511.678  500.742  509.539\n",
       "   ‚ãÆ                                 ‚ã±             ‚ãÆ               \n",
       " 522.559  518.192  530.008  512.315     515.162  522.757  505.014  513.793\n",
       " 508.399  510.44   519.091  512.039  ‚Ä¶  511.234  518.594  496.492  512.4\n",
       " 502.066  512.532  519.966  509.591     508.603  516.219  499.507  510.758\n",
       " 484.723  493.475  502.282  495.706     501.205  496.749  484.5    494.431\n",
       " 498.759  501.724  521.212  503.996     502.932  506.309  497.106  508.419\n",
       " 500.418  510.027  518.43   512.512     515.191  518.646  506.614  515.902\n",
       " 499.168  506.844  514.303  506.106  ‚Ä¶  509.358  508.772  499.897  504.395\n",
       " 508.263  520.537  522.667  509.787     514.355  520.065  505.641  517.933\n",
       " 483.851  504.238  506.899  490.97      491.386  498.944  485.049  496.811"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.@sync A_gpu * B_gpu # we need CUDA.@sync because GPU operations are typically asynchronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  42.645 ms (2 allocations: 16.00 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.774 ms (46 allocations: 1.12 KiB)\n"
     ]
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "@btime CUDA.@sync(A_cpu * B_cpu) setup=(A_cpu = rand(Float32, N,N); B_cpu = rand(Float32, N,N););\n",
    "@btime CUDA.@sync(A_gpu * B_gpu) setup=(A_gpu = CUDA.rand(N,N); B_gpu = CUDA.rand(N,N););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: `*` for `CuArray`s uses a cuBLAS kernel under the hood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More examples: Broadcasting, `map`, `reduce`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.@sync A_gpu .+ B_gpu; # runs on the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.@sync sqrt.(A_gpu.^2 + B_gpu.^2); # runs on the GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.@sync mapreduce(sin, +, A_gpu); # runs on the GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The power of simple GPU array programming can not be underestimated!** Entire codes (like deep learning frameworks etc.) could be ported to GPU without ever writing a single CUDA kernel manually.\n",
    "\n",
    "Of course, it isn't always as easy or performance can be improved by writing custom kernels. (‚Üí exercise **heat_diffusion**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Counter-example:\" Scalar indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore are only permitted from the REPL for prototyping purposes.\nIf you did intend to index this array, annotate the caller with @allowscalar.",
     "output_type": "error",
     "traceback": [
      "Scalar indexing is disallowed.\n",
      "Invocation of getindex resulted in scalar indexing of a GPU array.\n",
      "This is typically caused by calling an iterating implementation of a method.\n",
      "Such implementations *do not* execute on the GPU, but very slowly on the CPU,\n",
      "and therefore are only permitted from the REPL for prototyping purposes.\n",
      "If you did intend to index this array, annotate the caller with @allowscalar.\n",
      "\n",
      "Stacktrace:\n",
      " [1] error(s::String)\n",
      "   @ Base ./error.jl:35\n",
      " [2] assertscalar(op::String)\n",
      "   @ GPUArraysCore ~/.julia/packages/GPUArraysCore/uOYfN/src/GPUArraysCore.jl:103\n",
      " [3] getindex(A::CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, I::Int64)\n",
      "   @ GPUArrays ~/.julia/packages/GPUArrays/dAUOE/src/host/indexing.jl:48\n",
      " [4] top-level scope\n",
      "   @ ~/JuliaUCL24/notebooks/Day3/3_gpu_computing.ipynb:1"
     ]
    }
   ],
   "source": [
    "A_gpu[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.allowscalar(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore are only permitted from the REPL for prototyping purposes.\nIf you did intend to index this array, annotate the caller with @allowscalar.",
     "output_type": "error",
     "traceback": [
      "Scalar indexing is disallowed.\n",
      "Invocation of getindex resulted in scalar indexing of a GPU array.\n",
      "This is typically caused by calling an iterating implementation of a method.\n",
      "Such implementations *do not* execute on the GPU, but very slowly on the CPU,\n",
      "and therefore are only permitted from the REPL for prototyping purposes.\n",
      "If you did intend to index this array, annotate the caller with @allowscalar.\n",
      "\n",
      "Stacktrace:\n",
      " [1] error(s::String)\n",
      "   @ Base ./error.jl:35\n",
      " [2] assertscalar(op::String)\n",
      "   @ GPUArraysCore ~/.julia/packages/GPUArraysCore/uOYfN/src/GPUArraysCore.jl:103\n",
      " [3] getindex(A::CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}, I::Int64)\n",
      "   @ GPUArrays ~/.julia/packages/GPUArrays/dAUOE/src/host/indexing.jl:48\n",
      " [4] top-level scope\n",
      "   @ ~/JuliaUCL24/notebooks/Day3/3_gpu_computing.ipynb:1"
     ]
    }
   ],
   "source": [
    "A_gpu[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note**:\n",
    "\n",
    "* The access to an individual element in an array, e.g. scalar indexing, must not be used in array programming.\n",
    "* You must express arithmetic operations in terms of arrays and treat the `CuArray` array as a whole entity, e.g.\n",
    "\n",
    "```julia\n",
    "function gpu_broadcasting!(C, A, B)\n",
    "    CUDA.@sync C .= A .* B\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A few words on memory management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CuArray`s are managed by Julia's **garbage collector**. If they are unreachable, they will get cleaned up automatically during a GC run. However, keep in mind that the (CPU-focused) GC isn't good at sensing GPU memory pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective GPU memory usage: 99.13% (7.719 GiB/7.787 GiB)\n",
      "Memory pool usage: 64.000 MiB (7.188 GiB reserved)\n"
     ]
    }
   ],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_gpu = CUDA.rand(10_000_000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"38.147 MiB\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sizeof(x_gpu) |> Base.format_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective GPU memory usage: 99.22% (7.726 GiB/7.787 GiB)\n",
      "Memory pool usage: 102.147 MiB (7.188 GiB reserved)\n"
     ]
    }
   ],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_gpu = nothing; GC.gc(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective GPU memory usage: 99.48% (7.747 GiB/7.787 GiB)\n",
      "Memory pool usage: 70.147 MiB (7.188 GiB reserved)\n"
     ]
    }
   ],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default CUDA.jl uses a **memory pool** to speed up future allocations. So it might appear as if the objects have not been freed. (On Noctua 2 we have disabled it. You can disable the memory pool with `JULIA_CUDA_MEMORY_POOL=none`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `CUDA.unsafe_free!(x_gpu)` and `CUDA.reclaim()` to more aggressively suggest to release the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_gpu = CUDA.rand(10_000_000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective GPU memory usage: 99.37% (7.738 GiB/7.787 GiB)\n",
      "Memory pool usage: 108.294 MiB (7.188 GiB reserved)\n"
     ]
    }
   ],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CUDA.unsafe_free!(x_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective GPU memory usage: 99.63% (7.758 GiB/7.787 GiB)\n",
      "Memory pool usage: 70.147 MiB (7.188 GiB reserved)\n"
     ]
    }
   ],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, one must be careful with `CUDA.unsafe_free!` because one still has the handle `x_gpu` that now points to free'd memory. But it is fine and very useful in a pattern like this:\n",
    "\n",
    "```julia\n",
    "function myfunction(x::CuArray)\n",
    "    tmp_memory = similar(x)\n",
    "    expensive_operation!(x, tmp_memory)\n",
    "    CUDA.unsafe_free!(tmp_memory)\n",
    "    return x\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_gpu = nothing # to be safe :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel programming: Writing CUDA kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high-level array programming is most suitable for some types of computations. It may be less efficient, or even not possible for general applications.\n",
    "\n",
    "**CUDA kernel**: a function that will be executed by all *GPU threads* in parallel.\n",
    "\n",
    "Based on the index of a thread we can make them operate on different pieces of given data.\n",
    "\n",
    "(It might be helpful to think of the CUDA kernel as being the body of a loop.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cuda_kernel! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function cuda_kernel!(x)\n",
    "    i = threadIdx().x # the thread index (\"loop index\")\n",
    "    x[i] += 1\n",
    "    return nothing # CUDA kernels should never return anything\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can launch the kernel on the GPU with the `@cuda` macro (non-blocking, asynchronous):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.HostKernel for cuda_kernel!(CuDeviceVector{Float32, 1})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = CUDA.zeros(1024)\n",
    "\n",
    "CUDA.@sync @cuda threads=length(x) cuda_kernel!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " ‚ãÆ\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can imaging, kernel programming can become more difficult, especially if you care about performance. A few reasons:\n",
    "\n",
    "* you need to respect **hardware limitations** of the GPU, e.g. maximum number of GPU threads per block\n",
    "* **not all operations can readily be expressed as scalar kernels**, e.g. reduction\n",
    "* kernels execute on the GPU where the **Julia runtime isn't available**\n",
    "\n",
    "In particular due to the last point, kernel code has limitations\n",
    "  * no GC\n",
    "  * no `print` etc. (‚Üí `@cuprint`)\n",
    "  * code must be fully type inferred (no dynamic dispatch allowed)\n",
    "  * no `try ... catch ... end`\n",
    "  * ...\n",
    "\n",
    "**You can't just write arbitrary Julia code in kernels.** Fortunately though, many things just work and can get you far (see e.g. exercises)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Hardware limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "CuError",
     "evalue": "CUDA error: invalid argument (code 1, ERROR_INVALID_VALUE)",
     "output_type": "error",
     "traceback": [
      "CUDA error: invalid argument (code 1, ERROR_INVALID_VALUE)\n",
      "\n",
      "Stacktrace:\n",
      "  [1] throw_api_error(res::CUDA.cudaError_enum)\n",
      "    @ CUDA ~/.julia/packages/CUDA/rXson/lib/cudadrv/libcuda.jl:27\n",
      "  [2] check\n",
      "    @ ~/.julia/packages/CUDA/rXson/lib/cudadrv/libcuda.jl:34 [inlined]\n",
      "  [3] cuLaunchKernel\n",
      "    @ ~/.julia/packages/CUDA/rXson/lib/utils/call.jl:26 [inlined]\n",
      "  [4] (::CUDA.var\"#891#892\"{Bool, Int64, CuStream, CuFunction, CuDim3, CuDim3})(kernelParams::Vector{Ptr{Nothing}})\n",
      "    @ CUDA ~/.julia/packages/CUDA/rXson/lib/cudadrv/execution.jl:69\n",
      "  [5] macro expansion\n",
      "    @ ~/.julia/packages/CUDA/rXson/lib/cudadrv/execution.jl:33 [inlined]\n",
      "  [6] macro expansion\n",
      "    @ ./none:0 [inlined]\n",
      "  [7] pack_arguments(::CUDA.var\"#891#892\"{Bool, Int64, CuStream, CuFunction, CuDim3, CuDim3}, ::CUDA.KernelState, ::CuDeviceVector{Float32, 1})\n",
      "    @ CUDA ./none:0\n",
      "  [8] #launch#890\n",
      "    @ ~/.julia/packages/CUDA/rXson/lib/cudadrv/execution.jl:62 [inlined]\n",
      "  [9] #896\n",
      "    @ CUDA ~/.julia/packages/CUDA/rXson/lib/cudadrv/execution.jl:136 [inlined]\n",
      " [10] macro expansion\n",
      "    @ CUDA ~/.julia/packages/CUDA/rXson/lib/cudadrv/execution.jl:95 [inlined]\n",
      " [11] macro expansion\n",
      "    @ CUDA ./none:0 [inlined]\n",
      " [12] convert_arguments\n",
      "    @ CUDA ./none:0 [inlined]\n",
      " [13] #cudacall#895\n",
      "    @ CUDA ~/.julia/packages/CUDA/rXson/lib/cudadrv/execution.jl:135 [inlined]\n",
      " [14] cudacall\n",
      "    @ CUDA ~/.julia/packages/CUDA/rXson/lib/cudadrv/execution.jl:134 [inlined]\n",
      " [15] macro expansion\n",
      "    @ CUDA ~/.julia/packages/CUDA/rXson/src/compiler/execution.jl:258 [inlined]\n",
      " [16] macro expansion\n",
      "    @ CUDA ./none:0 [inlined]\n",
      " [17] call\n",
      "    @ CUDA ./none:0 [inlined]\n",
      " [18] #_#1098\n",
      "    @ CUDA ~/.julia/packages/CUDA/rXson/src/compiler/execution.jl:381 [inlined]\n",
      " [19] macro expansion\n",
      "    @ ~/.julia/packages/CUDA/rXson/src/compiler/execution.jl:106 [inlined]\n",
      " [20] macro expansion\n",
      "    @ ~/.julia/packages/CUDA/rXson/src/utilities.jl:35 [inlined]\n",
      " [21] top-level scope\n",
      "    @ ~/JuliaUCL24/notebooks/Day3/3_gpu_computing.ipynb:3"
     ]
    }
   ],
   "source": [
    "x = CUDA.zeros(1025) # one more element than before\n",
    "\n",
    "CUDA.@sync @cuda threads=length(x) cuda_kernel!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.attribute(device(), CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA programming model\n",
    "\n",
    "<!-- <img src=\"./imgs/CUDA_programming_model.png\" width=1024> -->\n",
    "<img src=\"imgs/cuda_prog_model.svg\" width=1024>\n",
    "\n",
    "Conceptual mapping:\n",
    "\n",
    "* **Grid** of blocks ‚Üí entire GPU\n",
    "* **Blocks** of threads ‚Üí SMs\n",
    "* **Threads** ‚Üí CUDA cores\n",
    "\n",
    "**Note**: up to three dimensions, $(x, y, z)$, can be used to organize the thread blocks and threads in each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cuda_kernel_blocks! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function cuda_kernel_blocks!(x)\n",
    "    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x # global thread index\n",
    "    if i <= length(x) # make sure that we're inbounds (c.f. \"loop\" iteration range)\n",
    "        @inbounds x[i] += 1\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = CUDA.zeros(1025);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**execution configuration** for a CUDA kernel:\n",
    "* `threads`: number of threads in each block\n",
    "* `blocks`: number of blocks in the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.HostKernel for cuda_kernel_blocks!(CuDeviceVector{Float32, 1})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.@sync @cuda threads=1024 blocks=2 cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1025-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " ‚ãÆ\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does our CUDA kernel compare to broadcasting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7.351 Œºs (28 allocations: 1.39 KiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  9.450 Œºs (2 allocations: 112 bytes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8.040 Œºs (2 allocations: 112 bytes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  16.594 Œºs (2 allocations: 112 bytes)\n"
     ]
    }
   ],
   "source": [
    "x = CUDA.rand(1024*1024);\n",
    "\n",
    "function add_one_broadcasting(x)\n",
    "    CUDA.@sync x .+ 1\n",
    "end\n",
    "\n",
    "# same CUDA kernel, but with different execution configurations\n",
    "function add_one_kernel_1024_1k(x)\n",
    "    CUDA.@sync @cuda threads=1024 blocks=1024 cuda_kernel_blocks!(x)\n",
    "end\n",
    "\n",
    "function add_one_kernel_256_4k(x)\n",
    "    CUDA.@sync @cuda threads=256 blocks=4*1024 cuda_kernel_blocks!(x)\n",
    "end\n",
    "\n",
    "function add_one_kernel_64_16k(x)\n",
    "    CUDA.@sync @cuda threads=64 blocks=16*1024 cuda_kernel_blocks!(x)\n",
    "end\n",
    "\n",
    "@btime add_one_broadcasting(x) setup=(x = CUDA.zeros(1024););\n",
    "@btime add_one_kernel_1024_1k(x) setup=(x = CUDA.zeros(1024););\n",
    "@btime add_one_kernel_256_4k(x) setup=(x = CUDA.zeros(1024););\n",
    "@btime add_one_kernel_64_16k(x) setup=(x = CUDA.zeros(1024););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of a CUDA kernel is affected by the execution configuration (because of the runtime scheduling of thread blocks and threads).\n",
    "\n",
    "How to obtain a good execution configuration for a CUDA kernel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplifying kernel launches: [Occupancy API](https://developer.nvidia.com/blog/cuda-pro-tip-occupancy-api-simplifies-launch-configuration/)\n",
    "\n",
    "Hardcoded execution configuration is not a good idea. A few reasons:\n",
    "\n",
    "* In reality, the actual maximal number of threads can depend on kernel details, like how many resources the kernel is using.\n",
    "* You might want to support different GPUs with different hardware limitations.\n",
    "\n",
    "**Occupancy** measures the ratio of the number of active *warps* per SM to the maximum number of possible warps per SM.\n",
    "\n",
    "*warp*: a group of 32 parallel threads executing the same instructions.\n",
    "\n",
    "* Low occupancy usually implies low performance (because of underutilized hardware resources).\n",
    "* High occupancy, however, may not guarantee the best performance.\n",
    "\n",
    "The occupancy API is an automatic tool that can be used to obtain *reasonably good* execution configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CUDA.HostKernel for cuda_kernel_blocks!(CuDeviceVector{Float32, 1})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kernel = @cuda launch=false cuda_kernel_blocks!(x) # don't launch the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(blocks = 92, threads = 768)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = CUDA.launch_configuration(kernel.fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the number `blocks` indicates how many blocks we would need to fully occupy the GPU. For a given input `x`, we might need fewer or more blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threads = min(length(x), config.threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1366"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "blocks = cld(length(x), threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launching the kernel with the dynamic launch parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel(x; threads, blocks) # calling `kernel` like a regular function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  862.500 ns (2 allocations: 112 bytes)\n"
     ]
    }
   ],
   "source": [
    "@btime kernel(x; threads=$threads, blocks=$blocks) setup=(x = CUDA.zeros(1024););"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `@code_*` for CPU there are `@device_code_*` macros for GPU. However, the GPU pendant for `@code_native` is `@device_code_ptx`.\n",
    "\n",
    "**PTX**: a low-level **p**arallel **t**hread e**x**ecution virtual machine and instruction set architecture used in NVIDIA CUDA programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTX CompilerJob of MethodInstance for cuda_kernel_blocks!(::CuDeviceVector{Float32, 1}) for sm_86\n",
      "\n",
      "MethodInstance for "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda_kernel_blocks!(::CuDeviceVector{Float32, 1})\n",
      "  from cuda_kernel_blocks!(\u001b[90mx\u001b[39m)\u001b[90m @\u001b[39m \u001b[90mMain\u001b[39m \u001b[90m~/JuliaUCL24/notebooks/Day3/\u001b[39m\u001b[90m\u001b[4m3_gpu_computing.ipynb:1\u001b[24m\u001b[39m\n",
      "Arguments\n",
      "  #self#\u001b[36m::Core.Const(cuda_kernel_blocks!)\u001b[39m\n",
      "  x\u001b[36m::CuDeviceVector{Float32, 1}\u001b[39m\n",
      "Locals\n",
      "  val\u001b[36m::Float32\u001b[39m\n",
      "  i\u001b[36m::Int64\u001b[39m\n",
      "Body\u001b[36m::Nothing\u001b[39m\n",
      "\u001b[90m1 ‚îÄ\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core.NewvarNode("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":(val))\n",
      "\u001b[90m‚îÇ  \u001b[39m %2  = Main.blockIdx()\u001b[36m::@NamedTuple{x::Int32, y::Int32, z::Int32}\u001b[39m\n",
      "\u001b[90m‚îÇ  \u001b[39m %3  = Base.getproperty(%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2, :x)\u001b[36m::Int32\u001b[39m\n",
      "\u001b[90m‚îÇ  \u001b[39m %4  = (%3 - 1)\u001b[36m::Int64\u001b[39m\n",
      "\u001b[90m‚îÇ  \u001b[39m %5  = Main.blockDim()\u001b[36m::@NamedTuple{x::Int32, y::Int32, z::Int32}\u001b[39m\n",
      "\u001b[90m‚îÇ  \u001b[39m %6  = Base.getproperty(%5, :x)\u001b[36m::Int32\u001b[39m\n",
      "\u001b[90m‚îÇ  \u001b[39m %7  = (%4 * %6)\u001b[36m::Int64\u001b[39m\n",
      "\u001b[90m‚îÇ  \u001b[39m %8  = Main.threadIdx()\u001b[36m::@NamedTuple{x::Int32, y::Int32, z::Int32}\u001b[39m\n",
      "\u001b[90m‚îÇ  \u001b[39m %9  = Base.getproperty(%8, :x)\u001b[36m::Int32\u001b[39m\n",
      "\u001b[90m‚îÇ  \u001b[39m       (i = %7 + %9)\n",
      "\u001b[90m‚îÇ  \u001b[39m %11 = i\u001b[36m::Int64\u001b[39m\n",
      "\u001b[90m‚îÇ  \u001b[39m %12 = Main.length(x)\u001b[36m::Int64\u001b[39m\n",
      "\u001b[90m‚îÇ  \u001b[39m %13 = (%11 <= %12)\u001b[36m::Bool\u001b[39m\n",
      "\u001b[90m‚îî‚îÄ‚îÄ\u001b[39m       goto #3 if not %13\n",
      "\u001b[90m2 ‚îÄ\u001b[39m       nothing\n",
      "\u001b[90m‚îÇ  \u001b[39m %16 = Base.getindex(x, i)\u001b[36m::Float32\u001b[39m\n",
      "\u001b[90m‚îÇ  \u001b[39m %17 = (%16 + 1)\u001b[36m::Float32\u001b[39m\n",
      "\u001b[90m‚îÇ  \u001b[39m       Base.setindex!(x, %17, i)\n",
      "\u001b[90m‚îÇ  \u001b[39m       (val = %17)\n",
      "\u001b[90m‚îÇ  \u001b[39m       nothing\n",
      "\u001b[90m‚îî‚îÄ‚îÄ\u001b[39m       val\n",
      "\u001b[90m3 ‚îÑ\u001b[39m       return Main.nothing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@device_code_warntype @cuda threads=1024 blocks=1024 cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "; PTX CompilerJob of MethodInstance for cuda_kernel_blocks!(::CuDeviceVector{Float32, 1}) for sm_86\n",
      "\u001b[95mdefine\u001b[39m \u001b[95mptx_kernel\u001b[39m \u001b[36mvoid\u001b[39m \u001b[93m@_Z19cuda_kernel_blocks_13CuDeviceArrayI7Float32Li1ELi1EE\u001b[39m\u001b[33m(\u001b[39m\u001b[33m{\u001b[39m \u001b[36mi64\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m, \u001b[36mi32\u001b[39m \u001b[33m}\u001b[39m \u001b[0m%state\u001b[0m, \u001b[33m{\u001b[39m \u001b[36mi8\u001b[39m \u001b[95maddrspace\u001b[39m\u001b[33m(\u001b[39m\u001b[33m1\u001b[39m\u001b[33m)\u001b[39m\u001b[0m*\u001b[0m, \u001b[36mi64\u001b[39m\u001b[0m, \u001b[33m[\u001b[39m\u001b[33m1\u001b[39m \u001b[0mx \u001b[36mi64\u001b[39m\u001b[33m]\u001b[39m\u001b[0m, \u001b[36mi64\u001b[39m \u001b[33m}\u001b[39m \u001b[0m%0\u001b[33m)\u001b[39m \u001b[95mlocal_unnamed_addr\u001b[39m \u001b[33m{\u001b[39m\n",
      "\u001b[91mconversion:\u001b[39m\n",
      "  \u001b[0m%.fca.3.extract \u001b[0m= \u001b[96m\u001b[1mextractvalue\u001b[22m\u001b[39m \u001b[33m{\u001b[39m \u001b[36mi8\u001b[39m \u001b[95maddrspace\u001b[39m\u001b[33m(\u001b[39m\u001b[33m1\u001b[39m\u001b[33m)\u001b[39m\u001b[0m*\u001b[0m, \u001b[36mi64\u001b[39m\u001b[0m, \u001b[33m[\u001b[39m\u001b[33m1\u001b[39m \u001b[0mx \u001b[36mi64\u001b[39m\u001b[33m]\u001b[39m\u001b[0m, \u001b[36mi64\u001b[39m \u001b[33m}\u001b[39m \u001b[0m%0\u001b[0m, \u001b[33m3\u001b[39m\n",
      "  \u001b[0m%1 \u001b[0m= \u001b[96m\u001b[1mcall\u001b[22m\u001b[39m \u001b[36mi32\u001b[39m \u001b[93m@llvm.nvvm.read.ptx.sreg.ctaid.x\u001b[39m\u001b[33m(\u001b[39m\u001b[33m)\u001b[39m\n",
      "  \u001b[0m%2 \u001b[0m= \u001b[96m\u001b[1mzext\u001b[22m\u001b[39m \u001b[36mi32\u001b[39m \u001b[0m%1 \u001b[95mto\u001b[39m \u001b[36mi64\u001b[39m\n",
      "  \u001b[0m%3 \u001b[0m= \u001b[96m\u001b[1mcall\u001b[22m\u001b[39m \u001b[36mi32\u001b[39m \u001b[93m@llvm.nvvm.read.ptx.sreg.ntid.x\u001b[39m\u001b[33m(\u001b[39m\u001b[33m)\u001b[39m\n",
      "  \u001b[0m%4 \u001b[0m= \u001b[96m\u001b[1mzext\u001b[22m\u001b[39m \u001b[36mi32\u001b[39m \u001b[0m%3 \u001b[95mto\u001b[39m \u001b[36mi64\u001b[39m\n",
      "  \u001b[0m%5 \u001b[0m= \u001b[96m\u001b[1mmul\u001b[22m\u001b[39m \u001b[95mnuw\u001b[39m \u001b[95mnsw\u001b[39m \u001b[36mi64\u001b[39m \u001b[0m%2\u001b[0m, \u001b[0m%4\n",
      "  \u001b[0m%6 \u001b[0m= \u001b[96m\u001b[1mcall\u001b[22m\u001b[39m \u001b[36mi32\u001b[39m \u001b[93m@llvm.nvvm.read.ptx.sreg.tid.x\u001b[39m\u001b[33m(\u001b[39m\u001b[33m)\u001b[39m\n",
      "  \u001b[0m%7 \u001b[0m= \u001b[96m\u001b[1madd\u001b[22m\u001b[39m \u001b[95mnuw\u001b[39m \u001b[95mnsw\u001b[39m \u001b[36mi32\u001b[39m \u001b[0m%6\u001b[0m, \u001b[33m1\u001b[39m\n",
      "  \u001b[0m%8 \u001b[0m= \u001b[96m\u001b[1mzext\u001b[22m\u001b[39m \u001b[36mi32\u001b[39m \u001b[0m%7 \u001b[95mto\u001b[39m \u001b[36mi64\u001b[39m\n",
      "  \u001b[0m%9 \u001b[0m= \u001b[96m\u001b[1madd\u001b[22m\u001b[39m \u001b[95mnuw\u001b[39m \u001b[95mnsw\u001b[39m \u001b[36mi64\u001b[39m \u001b[0m%5\u001b[0m, \u001b[0m%8\n",
      "  \u001b[0m%.not \u001b[0m= \u001b[96m\u001b[1micmp\u001b[22m\u001b[39m \u001b[96m\u001b[1msgt\u001b[22m\u001b[39m \u001b[36mi64\u001b[39m \u001b[0m%9\u001b[0m, \u001b[0m%.fca.3.extract\n",
      "  \u001b[96m\u001b[1mbr\u001b[22m\u001b[39m \u001b[36mi1\u001b[39m \u001b[0m%.not\u001b[0m, \u001b[36mlabel\u001b[39m \u001b[91m%L74\u001b[39m\u001b[0m, \u001b[36mlabel\u001b[39m \u001b[91m%L45\u001b[39m\n",
      "\n",
      "\u001b[91mL45:\u001b[39m                                              \u001b[90m; preds = %conversion\u001b[39m\n",
      "  \u001b[0m%.fca.0.extract \u001b[0m= \u001b[96m\u001b[1mextractvalue\u001b[22m\u001b[39m \u001b[33m{\u001b[39m \u001b[36mi8\u001b[39m \u001b[95maddrspace\u001b[39m\u001b[33m(\u001b[39m\u001b[33m1\u001b[39m\u001b[33m)\u001b[39m\u001b[0m*\u001b[0m, \u001b[36mi64\u001b[39m\u001b[0m, \u001b[33m[\u001b[39m\u001b[33m1\u001b[39m \u001b[0mx \u001b[36mi64\u001b[39m\u001b[33m]\u001b[39m\u001b[0m, \u001b[36mi64\u001b[39m \u001b[33m}\u001b[39m \u001b[0m%0\u001b[0m, \u001b[33m0\u001b[39m\n",
      "  \u001b[0m%10 \u001b[0m= \u001b[96m\u001b[1mbitcast\u001b[22m\u001b[39m \u001b[36mi8\u001b[39m \u001b[95maddrspace\u001b[39m\u001b[33m(\u001b[39m\u001b[33m1\u001b[39m\u001b[33m)\u001b[39m\u001b[0m* \u001b[0m%.fca.0.extract \u001b[95mto\u001b[39m \u001b[36mfloat\u001b[39m \u001b[95maddrspace\u001b[39m\u001b[33m(\u001b[39m\u001b[33m1\u001b[39m\u001b[33m)\u001b[39m\u001b[0m*\n",
      "  \u001b[0m%11 \u001b[0m= \u001b[96m\u001b[1madd\u001b[22m\u001b[39m \u001b[95mnsw\u001b[39m \u001b[36mi64\u001b[39m \u001b[0m%9\u001b[0m, \u001b[33m-1\u001b[39m\n",
      "  \u001b[0m%12 \u001b[0m= \u001b[96m\u001b[1mgetelementptr\u001b[22m\u001b[39m \u001b[95minbounds\u001b[39m \u001b[36mfloat\u001b[39m\u001b[0m, \u001b[36mfloat\u001b[39m \u001b[95maddrspace\u001b[39m\u001b[33m(\u001b[39m\u001b[33m1\u001b[39m\u001b[33m)\u001b[39m\u001b[0m* \u001b[0m%10\u001b[0m, \u001b[36mi64\u001b[39m \u001b[0m%11\n",
      "  \u001b[0m%13 \u001b[0m= \u001b[96m\u001b[1mload\u001b[22m\u001b[39m \u001b[36mfloat\u001b[39m\u001b[0m, \u001b[36mfloat\u001b[39m \u001b[95maddrspace\u001b[39m\u001b[33m(\u001b[39m\u001b[33m1\u001b[39m\u001b[33m)\u001b[39m\u001b[0m* \u001b[0m%12\u001b[0m, \u001b[95malign\u001b[39m \u001b[33m4\u001b[39m\n",
      "  \u001b[0m%14 \u001b[0m= \u001b[96m\u001b[1mfadd\u001b[22m\u001b[39m \u001b[36mfloat\u001b[39m \u001b[0m%13\u001b[0m, \u001b[33m1.000000e+00\u001b[39m\n",
      "  \u001b[96m\u001b[1mstore\u001b[22m\u001b[39m \u001b[36mfloat\u001b[39m \u001b[0m%14\u001b[0m, \u001b[36mfloat\u001b[39m \u001b[95maddrspace\u001b[39m\u001b[33m(\u001b[39m\u001b[33m1\u001b[39m\u001b[33m)\u001b[39m\u001b[0m* \u001b[0m%12\u001b[0m, \u001b[95malign\u001b[39m \u001b[33m4\u001b[39m\n",
      "  \u001b[96m\u001b[1mbr\u001b[22m\u001b[39m \u001b[36mlabel\u001b[39m \u001b[91m%L74\u001b[39m\n",
      "\n",
      "\u001b[91mL74:\u001b[39m                                              \u001b[90m; preds = %L45, %conversion\u001b[39m\n",
      "  \u001b[96m\u001b[1mret\u001b[22m\u001b[39m \u001b[36mvoid\u001b[39m\n",
      "\u001b[33m}\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@device_code_llvm debuginfo=:none @cuda threads=1024 blocks=1024 cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// PTX CompilerJob of MethodInstance for cuda_kernel_blocks!(::CuDeviceVector{Float32, 1}) for sm_86\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m//\u001b[39;49;00m\n",
      "\u001b[90m// Generated by LLVM NVPTX Back-End\u001b[39;49;00m\n",
      "\u001b[90m//\u001b[39;49;00m\n",
      "\n",
      "\u001b[94m.version\u001b[39;49;00m \u001b[94m8.2\u001b[39;49;00m\n",
      "\u001b[94m.target\u001b[39;49;00m \u001b[91msm_86\u001b[39;49;00m\n",
      "\u001b[94m.address_size\u001b[39;49;00m \u001b[94m64\u001b[39;49;00m\n",
      "\n",
      "\t\u001b[90m// .globl\t_Z19cuda_kernel_blocks_13CuDeviceArrayI7Float32Li1ELi1EE // -- Begin function _Z19cuda_kernel_blocks_13CuDeviceArrayI7Float32Li1ELi1EE\u001b[39;49;00m\n",
      "                                        \u001b[90m// @_Z19cuda_kernel_blocks_13CuDeviceArrayI7Float32Li1ELi1EE\u001b[39;49;00m\n",
      "\u001b[94m.visible\u001b[39;49;00m \u001b[94m.entry\u001b[39;49;00m \u001b[04m\u001b[91m_\u001b[39;49;00m\u001b[91mZ19cuda_kernel_blocks_13CuDeviceArrayI7Float32Li1ELi1EE\u001b[39;49;00m(\n",
      "\t\u001b[94m.param\u001b[39;49;00m \u001b[94m.align\u001b[39;49;00m \u001b[94m8\u001b[39;49;00m \u001b[96m.b8\u001b[39;49;00m \u001b[04m\u001b[91m_\u001b[39;49;00m\u001b[91mZ19cuda_kernel_blocks_13CuDeviceArrayI7Float32Li1ELi1EE_param_0\u001b[39;49;00m[\u001b[94m16\u001b[39;49;00m],\n",
      "\t\u001b[94m.param\u001b[39;49;00m \u001b[94m.align\u001b[39;49;00m \u001b[94m8\u001b[39;49;00m \u001b[96m.b8\u001b[39;49;00m \u001b[04m\u001b[91m_\u001b[39;49;00m\u001b[91mZ19cuda_kernel_blocks_13CuDeviceArrayI7Float32Li1ELi1EE_param_1\u001b[39;49;00m[\u001b[94m32\u001b[39;49;00m]\n",
      ")\n",
      "{\n",
      "\t\u001b[94m.reg\u001b[39;49;00m \u001b[96m.pred\u001b[39;49;00m \t\u001b[91m%p\u001b[39;49;00m<\u001b[94m2\u001b[39;49;00m>;\n",
      "\t\u001b[94m.reg\u001b[39;49;00m \u001b[96m.b32\u001b[39;49;00m \t\u001b[91m%r\u001b[39;49;00m<\u001b[94m5\u001b[39;49;00m>;\n",
      "\t\u001b[94m.reg\u001b[39;49;00m \u001b[96m.f32\u001b[39;49;00m \t\u001b[91m%f\u001b[39;49;00m<\u001b[94m3\u001b[39;49;00m>;\n",
      "\t\u001b[94m.reg\u001b[39;49;00m \u001b[96m.b64\u001b[39;49;00m \t\u001b[91m%rd\u001b[39;49;00m<\u001b[94m11\u001b[39;49;00m>;\n",
      "\n",
      "\u001b[90m// %bb.0:                               // %conversion\u001b[39;49;00m\n",
      "\t\u001b[94mld.param.u64\u001b[39;49;00m \t\u001b[91m%rd3\u001b[39;49;00m, [\u001b[04m\u001b[91m_\u001b[39;49;00m\u001b[91mZ19cuda_kernel_blocks_13CuDeviceArrayI7Float32Li1ELi1EE_param_1\u001b[39;49;00m+\u001b[94m24\u001b[39;49;00m];\n",
      "\t\u001b[94mmov.u32\u001b[39;49;00m \t\u001b[91m%r1\u001b[39;49;00m, \u001b[91m%ctaid\u001b[39;49;00m.\u001b[91mx\u001b[39;49;00m;\n",
      "\t\u001b[94mmov.u32\u001b[39;49;00m \t\u001b[91m%r2\u001b[39;49;00m, \u001b[91m%ntid\u001b[39;49;00m.\u001b[91mx\u001b[39;49;00m;\n",
      "\t\u001b[94mmul.wide.u32\u001b[39;49;00m \t\u001b[91m%rd4\u001b[39;49;00m, \u001b[91m%r1\u001b[39;49;00m, \u001b[91m%r2\u001b[39;49;00m;\n",
      "\t\u001b[94mmov.u32\u001b[39;49;00m \t\u001b[91m%r3\u001b[39;49;00m, \u001b[91m%tid\u001b[39;49;00m.\u001b[91mx\u001b[39;49;00m;\n",
      "\t\u001b[94madd.s32\u001b[39;49;00m \t\u001b[91m%r4\u001b[39;49;00m, \u001b[91m%r3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m;\n",
      "\t\u001b[94mcvt.u64.u32\u001b[39;49;00m \t\u001b[91m%rd5\u001b[39;49;00m, \u001b[91m%r4\u001b[39;49;00m;\n",
      "\t\u001b[94madd.s64\u001b[39;49;00m \t\u001b[91m%rd6\u001b[39;49;00m, \u001b[91m%rd4\u001b[39;49;00m, \u001b[91m%rd5\u001b[39;49;00m;\n",
      "\t\u001b[94msetp.gt.s64\u001b[39;49;00m \t\u001b[91m%p1\u001b[39;49;00m, \u001b[91m%rd6\u001b[39;49;00m, \u001b[91m%rd3\u001b[39;49;00m;\n",
      "\t@\u001b[91m%p1\u001b[39;49;00m \u001b[91mbra\u001b[39;49;00m \t\u001b[04m\u001b[91m$\u001b[39;49;00m\u001b[91mL__BB0_2\u001b[39;49;00m;\n",
      "\u001b[90m// %bb.1:                               // %L45\u001b[39;49;00m\n",
      "\t\u001b[94mld.param.u64\u001b[39;49;00m \t\u001b[91m%rd2\u001b[39;49;00m, [\u001b[04m\u001b[91m_\u001b[39;49;00m\u001b[91mZ19cuda_kernel_blocks_13CuDeviceArrayI7Float32Li1ELi1EE_param_1\u001b[39;49;00m];\n",
      "\t\u001b[94mcvt.u64.u32\u001b[39;49;00m \t\u001b[91m%rd7\u001b[39;49;00m, \u001b[91m%r3\u001b[39;49;00m;\n",
      "\t\u001b[94madd.s64\u001b[39;49;00m \t\u001b[91m%rd8\u001b[39;49;00m, \u001b[91m%rd4\u001b[39;49;00m, \u001b[91m%rd7\u001b[39;49;00m;\n",
      "\t\u001b[94mshl.b64\u001b[39;49;00m \t\u001b[91m%rd9\u001b[39;49;00m, \u001b[91m%rd8\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m;\n",
      "\t\u001b[94madd.s64\u001b[39;49;00m \t\u001b[91m%rd10\u001b[39;49;00m, \u001b[91m%rd9\u001b[39;49;00m, \u001b[91m%rd2\u001b[39;49;00m;\n",
      "\t\u001b[94madd.s64\u001b[39;49;00m \t\u001b[91m%rd1\u001b[39;49;00m, \u001b[91m%rd10\u001b[39;49;00m, -\u001b[94m4\u001b[39;49;00m;\n",
      "\t\u001b[94mld.global.f32\u001b[39;49;00m \t\u001b[91m%f1\u001b[39;49;00m, [\u001b[91m%rd1\u001b[39;49;00m+\u001b[94m4\u001b[39;49;00m];\n",
      "\t\u001b[94madd.f32\u001b[39;49;00m \t\u001b[91m%f2\u001b[39;49;00m, \u001b[91m%f1\u001b[39;49;00m, \u001b[94m0f\u001b[39;49;00m\u001b[94m3F\u001b[39;49;00m\u001b[04m\u001b[91m8\u001b[39;49;00m\u001b[94m00000\u001b[39;49;00m;\n",
      "\t\u001b[94mst.global.f32\u001b[39;49;00m \t[\u001b[91m%rd1\u001b[39;49;00m+\u001b[94m4\u001b[39;49;00m], \u001b[91m%f2\u001b[39;49;00m;\n",
      "\u001b[04m\u001b[91m$\u001b[39;49;00m\u001b[91mL__BB0_2\u001b[39;49;00m\u001b[04m\u001b[91m:\u001b[39;49;00m                              \u001b[90m// %L74\u001b[39;49;00m\n",
      "\t\u001b[91mret\u001b[39;49;00m;\n",
      "                                        \u001b[90m// -- End function\u001b[39;49;00m\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "@device_code_ptx @cuda threads=1024 blocks=1024 cuda_kernel_blocks!(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPUs in one compute node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Note: You can't run this part because you only have a single device in this session.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **4x NVIDIA A100 GPUs** in a regular Noctua 2 GPU node:\n",
    "\n",
    "<!-- <img src=\"./imgs/Noctua2_GPU_node.png\" width=320px> -->\n",
    "<img src=\"imgs/Noctua2_GPU_node.svg\" width=320px>\n",
    "\n",
    "Since each Julia task gets its own local CUDA execution environment, it is easy to utilize multi-GPUs in one compute node to perform computations in parallel by launching multiple Julia tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base.Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will use `Threads.@spawn` (since multi-threading support is a rather recent addition to CUDA.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multi_gpu (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function gpu_computation(A, B, C)\n",
    "    for i in 1:512\n",
    "        C = A * B\n",
    "        A = B * C\n",
    "        B = C * A\n",
    "    end\n",
    "    sin.(B)\n",
    "    return B\n",
    "end\n",
    "\n",
    "function multi_gpu()\n",
    "    n = 4096\n",
    "    @sync begin\n",
    "        # Julia task for the 1st GPU\n",
    "        @spawn begin\n",
    "            device!(0) # first GPU\n",
    "            A = CUDA.rand(n, n)\n",
    "            B = CUDA.rand(n, n)\n",
    "            C = CUDA.zeros(n, n)\n",
    "            println(\"GPU 1: running gpu_computation\")\n",
    "            CUDA.@sync gpu_computation(A,B,C)\n",
    "            println(\"GPU 1: done\")\n",
    "        end\n",
    "        # Julia task for the 2nd GPU\n",
    "        @spawn begin\n",
    "            device!(1) # second GPU\n",
    "            A = CUDA.rand(n, n)\n",
    "            B = CUDA.rand(n, n)\n",
    "            C = CUDA.zeros(n, n)\n",
    "            println(\"GPU 2: running gpu_computation\")\n",
    "            CUDA.@sync gpu_computation(A,B,C)\n",
    "            println(\"GPU 2: done\")\n",
    "        end\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 1: running gpu_computation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 1: done\n"
     ]
    },
    {
     "ename": "CompositeException",
     "evalue": "TaskFailedException\n\n    nested task error: CUDA error: invalid device ordinal (code 101, ERROR_INVALID_DEVICE)\n    Stacktrace:\n     [1] throw_api_error(res::CUDA.cudaError_enum)\n       @ CUDA ~/.julia/packages/CUDA/rXson/lib/cudadrv/libcuda.jl:27\n     [2] check\n       @ ~/.julia/packages/CUDA/rXson/lib/cudadrv/libcuda.jl:34 [inlined]\n     [3] cuDeviceGet\n       @ ~/.julia/packages/CUDA/rXson/lib/utils/call.jl:26 [inlined]\n     [4] CuDevice\n       @ ~/.julia/packages/CUDA/rXson/lib/cudadrv/devices.jl:17 [inlined]\n     [5] device!(dev::CuDevice, flags::Nothing) (repeats 2 times)\n       @ CUDA ~/.julia/packages/CUDA/rXson/lib/cudadrv/state.jl:336 [inlined]\n     [6] (::var\"#12#14\"{Int64})()\n       @ Main ~/JuliaUCL24/notebooks/Day3/3_gpu_computing.ipynb:26",
     "output_type": "error",
     "traceback": [
      "TaskFailedException\n",
      "\n",
      "    nested task error: CUDA error: invalid device ordinal (code 101, ERROR_INVALID_DEVICE)\n",
      "    Stacktrace:\n",
      "     [1] throw_api_error(res::CUDA.cudaError_enum)\n",
      "       @ CUDA ~/.julia/packages/CUDA/rXson/lib/cudadrv/libcuda.jl:27\n",
      "     [2] check\n",
      "       @ ~/.julia/packages/CUDA/rXson/lib/cudadrv/libcuda.jl:34 [inlined]\n",
      "     [3] cuDeviceGet\n",
      "       @ ~/.julia/packages/CUDA/rXson/lib/utils/call.jl:26 [inlined]\n",
      "     [4] CuDevice\n",
      "       @ ~/.julia/packages/CUDA/rXson/lib/cudadrv/devices.jl:17 [inlined]\n",
      "     [5] device!(dev::CuDevice, flags::Nothing) (repeats 2 times)\n",
      "       @ CUDA ~/.julia/packages/CUDA/rXson/lib/cudadrv/state.jl:336 [inlined]\n",
      "     [6] (::var\"#12#14\"{Int64})()\n",
      "       @ Main ~/JuliaUCL24/notebooks/Day3/3_gpu_computing.ipynb:26\n",
      "\n",
      "Stacktrace:\n",
      " [1] sync_end(c::Channel{Any})\n",
      "   @ Base ./task.jl:448\n",
      " [2] macro expansion\n",
      "   @ ./task.jl:480 [inlined]\n",
      " [3] multi_gpu()\n",
      "   @ Main ~/JuliaUCL24/notebooks/Day3/3_gpu_computing.ipynb:13\n",
      " [4] top-level scope\n",
      "   @ ~/JuliaUCL24/notebooks/Day3/3_gpu_computing.ipynb:1"
     ]
    }
   ],
   "source": [
    "multi_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Effective GPU memory usage: 99.07% (7.715 GiB/7.787 GiB)\n",
      "Memory pool usage: 1.848 GiB (7.188 GiB reserved)\n"
     ]
    }
   ],
   "source": [
    "# call CUDA.memory_status() for all GPUs\n",
    "for dev in devices()\n",
    "    device!(dev)\n",
    "    println()\n",
    "    CUDA.memory_status()\n",
    "end\n",
    "device!(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking and Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuDevice(0): NVIDIA GeForce RTX 3070 Ti Laptop GPU"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device!(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  43.354 Œºs (29 allocations: 1.86 KiB)\n"
     ]
    }
   ],
   "source": [
    "A = CUDA.rand(1024, 1024)\n",
    "B = CUDA.rand(1024, 1024)\n",
    "\n",
    "@btime CUDA.@sync A .* B;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \"allocations\" here means CPU allocations. For GPU allocations you can e.g. use `CUDA.@time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000298 seconds (32 CPU allocations: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.906 KiB)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (1 GPU allocation: 4.000 MiB, 4.73% memmgmt time)\n"
     ]
    }
   ],
   "source": [
    "CUDA.@time A .* B;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrated profiler: `CUDA.@profile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Profiler ran for 218.87 ¬µs, capturing 11 events.\n",
       "\n",
       "Host-side activity: calling CUDA APIs took 47.68 ¬µs (21.79% of the trace)\n",
       "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
       "‚îÇ\u001b[1m Time (%) \u001b[0m‚îÇ\u001b[1m Total time \u001b[0m‚îÇ\u001b[1m Calls \u001b[0m‚îÇ\u001b[1m Name                    \u001b[0m‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ   11.22% ‚îÇ\u001b[31m   24.56 ¬µs \u001b[0m‚îÇ     1 ‚îÇ\u001b[1m cuLaunchKernel          \u001b[0m‚îÇ\n",
       "‚îÇ    5.45% ‚îÇ   11.92 ¬µs ‚îÇ     1 ‚îÇ cuMemAllocFromPoolAsync ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "\n",
       "Device-side activity: GPU was busy for 39.1 ¬µs (17.86% of the trace)\n",
       "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
       "‚îÇ\u001b[1m Time (%) \u001b[0m‚îÇ\u001b[1m Total time \u001b[0m‚îÇ\u001b[1m Calls \u001b[0m‚îÇ\u001b[1m Name                                                                                                                                                                                                                                            \u001b[0m‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ   17.86% ‚îÇ\u001b[31m    39.1 ¬µs \u001b[0m‚îÇ     1 ‚îÇ\u001b[1m _Z16broadcast_kernel15CuKernelContext13CuDeviceArrayI7Float32Li2ELi1EE11BroadcastedI12CuArrayStyleILi2EE5TupleI5OneToI5Int64ES5_IS6_EE1_S4_I8ExtrudedIS0_IS1_Li2ELi1EES4_I4BoolS9_ES4_IS6_S6_EES8_IS0_IS1_Li2ELi1EES4_IS9_S9_ES4_IS6_S6_EEEES6_ \u001b[0m‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.@profile A .* B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Profiler ran for 287.77 ¬µs, capturing 11 events.\n",
       "\n",
       "Host-side activity: calling CUDA APIs took 65.57 ¬µs (22.78% of the trace)\n",
       "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
       "‚îÇ\u001b[1m ID \u001b[0m‚îÇ\u001b[1m    Start \u001b[0m‚îÇ\u001b[1m     Time \u001b[0m‚îÇ\u001b[1m Name                    \u001b[0m‚îÇ\u001b[1m                  Details \u001b[0m‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ  5 ‚îÇ 25.03 ¬µs ‚îÇ\u001b[31m 37.91 ¬µs \u001b[0m‚îÇ\u001b[1m cuMemAllocFromPoolAsync \u001b[0m‚îÇ 4.000 MiB, device memory ‚îÇ\n",
       "‚îÇ  9 ‚îÇ 95.13 ¬µs ‚îÇ  24.8 ¬µs ‚îÇ cuLaunchKernel          ‚îÇ                        - ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "\n",
       "Device-side activity: GPU was busy for 40.05 ¬µs (13.92% of the trace)\n",
       "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
       "‚îÇ\u001b[1m ID \u001b[0m‚îÇ\u001b[1m     Start \u001b[0m‚îÇ\u001b[1m     Time \u001b[0m‚îÇ\u001b[1m Threads \u001b[0m‚îÇ\u001b[1m Blocks \u001b[0m‚îÇ\u001b[1m Regs \u001b[0m‚îÇ\u001b[1m Name                                                                                                                                                                                                                                            \u001b[0m‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ  9 ‚îÇ 246.29 ¬µs ‚îÇ\u001b[31m 40.05 ¬µs \u001b[0m‚îÇ     768 ‚îÇ     92 ‚îÇ   32 ‚îÇ\u001b[1m _Z16broadcast_kernel15CuKernelContext13CuDeviceArrayI7Float32Li2ELi1EE11BroadcastedI12CuArrayStyleILi2EE5TupleI5OneToI5Int64ES5_IS6_EE1_S4_I8ExtrudedIS0_IS1_Li2ELi1EES4_I4BoolS9_ES4_IS6_S6_EES8_IS0_IS1_Li2ELi1EES4_IS9_S9_ES4_IS6_S6_EEEES6_ \u001b[0m‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CUDA.@profile trace=true A .* B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External profiler: [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems) (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://cuda.juliagpu.org/stable/development/profiling/#External-profilers\n",
    "\n",
    "**Command**: `CUDA.@profile external=true`\n",
    "\n",
    "Use [**NVTX.jl**](https://github.com/JuliaGPU/NVTX.jl) to annotate (i.e. label and colorize) code blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/nsight_systems.png\" width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case study: Three ways to SAXPY on the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SAXPY** = **S**ingle precision **A** times **X** **P**lus **Y**\n",
    "\n",
    "‚Üí exercise **saxpy_gpu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/a100_saxpy_results.png\" width=1024px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core messages of this notebook\n",
    "\n",
    "* GPU architecture is optimal for **high throughput computations using millions of threads**.\n",
    "* For good performance the communication between host and GPU device(s) must be minimized.\n",
    "* CUDA.jl enables to program NVIDIA GPUs in Julia\n",
    "    - high-level array abstraction with `CuArray`\n",
    "        * convenient for some types of computation\n",
    "        * reasonably good performance is achievable\n",
    "    - writing custom CUDA kernels\n",
    "        * flexible for general computations\n",
    "        * **CUDA programming model**\n",
    "            - **Grid of blocks** ‚Üí entire GPU\n",
    "            - **Blocks of threads** ‚Üí SMs\n",
    "            - **Threads** ‚Üí CUDA cores\n",
    "        * the performance is influenced by **the execution configuration**, e.g. number of thread blocks and number of threads per block\n",
    "        * use Occupancy API to obtain an optimal execution configuration for a CUDA kernel\n",
    "    - using the vendor GPU libraries, e.g. cuBLAS, if possible\n",
    "* Multi-GPUs in one compute node can be utilized in parallel with multiple Julia tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
