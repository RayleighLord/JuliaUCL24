{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, [BenchmarkTools.jl](https://github.com/JuliaCI/BenchmarkTools.jl) provides tools to micro-benchmark specific functions. However, sometimes we want to zoom out and identify bottlenecks in a larger code.\n",
    "\n",
    "There are two qualitatively different techniques:\n",
    "* **Instrumented** profiling\n",
    "* **Statistical** profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Matrix-Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matmul (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function matmul(n, k=n)\n",
    "    A = rand(n, k)\n",
    "    B = rand(k, n)\n",
    "    C = zeros(n, n)\n",
    "    for n in axes(C, 2)\n",
    "        for m in axes(C, 1)\n",
    "            Cmn = zero(eltype(C))\n",
    "            for k in axes(A, 2)\n",
    "                tmp = A[m, k] * B[k, n]\n",
    "                Cmn += tmp\n",
    "            end\n",
    "            C[m, n] = Cmn\n",
    "        end\n",
    "    end\n",
    "    return C\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "matmul(10, 5); # trigger compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrumented Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to modify our code and explicitly add profiling bits to it. Specifically, we'll use [TimerOutputs.jl](https://github.com/KristofferC/TimerOutputs.jl).\n",
    "\n",
    "**Pros**\n",
    "* Accurate and complete performance statistics\n",
    "\n",
    "**Cons**\n",
    "* Need to modify the source code\n",
    "* Some overhead\n",
    "* Limited support for multithreading ([TrackingTimers.jl](https://github.com/ericphanson/TrackingTimers.jl) may be an alternative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using TimerOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matmul_instrumented (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function matmul_instrumented(n, k=n)\n",
    "    @timeit \"initialize matrices\" begin\n",
    "        @timeit \"init A\" A = rand(n, k)\n",
    "        @timeit \"init B\" B = rand(k, n)\n",
    "        @timeit \"init C\" C = zeros(n, n)\n",
    "    end\n",
    "    # simple matmul implementation\n",
    "    @timeit \"matmul\" for n in axes(C, 2)\n",
    "        for m in axes(C, 1)\n",
    "            Cmn = zero(eltype(C))\n",
    "            for k in axes(A, 2)\n",
    "                @timeit \"mul\" tmp = A[m, k] * B[k, n]\n",
    "                @timeit \"add\" Cmn += tmp\n",
    "            end\n",
    "            C[m, n] = Cmn\n",
    "        end\n",
    "    end\n",
    "    return C\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────────────────\u001b[22m\n",
       "\u001b[0m\u001b[1m                               \u001b[22m         Time                    Allocations      \n",
       "                               ───────────────────────   ────────────────────────\n",
       "       Tot / % measured:            12.2s /   0.8%           31.5MiB /   0.3%    \n",
       "\n",
       " Section               ncalls     time    %tot     avg     alloc    %tot      avg\n",
       " ────────────────────────────────────────────────────────────────────────────────\n",
       " matmul                     1   91.7ms  100.0%  91.7ms   1.47KiB    1.5%  1.47KiB\n",
       "   mul                   100k   4.19ms    4.6%  41.9ns     0.00B    0.0%    0.00B\n",
       "   add                   100k   3.81ms    4.2%  38.1ns     0.00B    0.0%    0.00B\n",
       " initialize matrices        1   10.9μs    0.0%  10.9μs   96.2KiB   98.5%  96.2KiB\n",
       "   init A                   1   3.71μs    0.0%  3.71μs   7.94KiB    8.1%  7.94KiB\n",
       "   init C                   1   2.60μs    0.0%  2.60μs   78.2KiB   80.0%  78.2KiB\n",
       "   init B                   1   1.13μs    0.0%  1.13μs   7.94KiB    8.1%  7.94KiB\n",
       "\u001b[0m\u001b[1m ────────────────────────────────────────────────────────────────────────────────\u001b[22m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to = TimerOutputs.get_defaulttimer()\n",
    "# TimerOutputs.reset_timer!(to)\n",
    "matmul_instrumented(100, 10);\n",
    "to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to repeatedly record the state of the program (i.e. which line or function is currently executing) while it is running with a given sample rate.\n",
    "\n",
    "Julia has built-in [statistical profilers](https://goo.gl/Ycz4Td) in the standard library [`Profile`](https://docs.julialang.org/en/v1/stdlib/Profile/) (see also [here](https://docs.julialang.org/en/v1/manual/profile/)). We will use these profilers to identify the parts of our `matmul` function that have\n",
    "* the highest computation time\n",
    "* make the most / the biggest allocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profiling is as simple as prepending the function call by the `@profile` macro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Profile\n",
    "Profile.clear() # clean up old profiling data\n",
    "@profile matmul(1000, 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic way to analyze the profiling results is `Profile.print()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Count  Overhead File                    Line Function\n",
      " =====  ======== ====                    ==== ========\n",
      "    63         0 @Base/Base.jl            495 include(mod::Module, _path::Str...\n",
      "    63         1 @Base/boot.jl            385 eval\n",
      "    63         0 @Base/client.jl          552 _start()\n",
      "    63         0 @Base/client.jl          318 exec_options(opts::Base.JLOptions)\n",
      "    63         0 @Base/essentials.jl      887 #invokelatest#2\n",
      "    63         0 @Base/essentials.jl      884 invokelatest\n",
      "     2         2 @Base/float.jl           411 *\n",
      "     1         1 @Base/float.jl           409 +\n",
      "    63         0 @Base/loading.jl        2130 _include(mapexpr::Function, mod...\n",
      "    63         0 @Base/loading.jl        2070 include_string(mapexpr::typeof(...\n",
      "    63         0 ...otebook/notebook.jl    32 top-level scope\n",
      "    63         0 @JSONRPC/src/typed.jl     67 dispatch_msg(x::VSCodeServer.JS...\n",
      "    63         0 ...eServer/src/repl.jl   274 withpath(f::VSCodeServer.var\"#2...\n",
      "    63         0 ...c/serve_notebook.jl    19 (::VSCodeServer.var\"#208#209\"{V...\n",
      "    63         0 ...c/serve_notebook.jl   139 serve_notebook(pipename::String...\n",
      "    63         0 ...c/serve_notebook.jl    13 notebook_runcell_request(conn::...\n",
      "    63         0 ...c/serve_notebook.jl    75 kwcall(::@NamedTuple{crashrepor...\n",
      "    18        18 ...2/4_profiling.ipynb     ? matmul(n::Int64, k::Int64)\n",
      "     2         0 ...2/4_profiling.ipynb     9 matmul(n::Int64, k::Int64)\n",
      "     1         0 ...2/4_profiling.ipynb    10 matmul(n::Int64, k::Int64)\n",
      "    41        40 ...2/4_profiling.ipynb    11 matmul(n::Int64, k::Int64)\n",
      "Total snapshots: 64. Utilization: 100% across all threads and tasks. Use the `groupby` kwarg to break down by thread and/or task.\n"
     ]
    }
   ],
   "source": [
    "Profile.print(; threads=1, format=:flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A much nicer way to analyze the profiling results is to visualize them as a flame graph. In principle, one can choose from a number of visualization tools. To name a few:\n",
    "\n",
    "* [ProfileView.jl](https://github.com/timholy/ProfileView.jl)\n",
    "* [ProfileVega.jl](https://github.com/davidanthoff/ProfileVega.jl)\n",
    "* [ProfileSVG.jl](https://github.com/kimikage/ProfileSVG.jl)\n",
    "* [PProf.jl](https://github.com/JuliaPerf/PProf.jl)\n",
    "* ...\n",
    "\n",
    "However, personally, I recommend to use the [Julia extension for Visual Studio Code (VS Code)](https://www.julia-vscode.org/) which has built-in [profiling visualization capabilities](https://www.julia-vscode.org/docs/stable/userguide/profiler/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel VTune Profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [IntelITT.jl](https://github.com/JuliaPerf/IntelITT.jl) for instrumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/vtune_gui_flamegraph.png\" width=800px>\n",
    "\n",
    "(source: https://github.com/carstenbauer/julia-intelvtune, see also: https://juliahpc.github.io/JuliaOnHPCClusters/user_hpcprofiling/intel_vtune/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware-Level Performance Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we have considered **software** profiling options. Another approach to assessing the performance of a (piece of) Julia code are **hardware** performance counters, which are built into most modern CPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [LIKWID.jl](https://github.com/JuliaPerf/LIKWID.jl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/likwidjl_logo.png\" width=300px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilize those counters in Julia, one can use **[LIKWID.jl](https://github.com/JuliaPerf/LIKWID.jl)** which is a wrapper around the performance monitoring and benchmarking suite [LIKWID](https://github.com/RRZE-HPC/likwid) (Like I Knew What I'm Doing) by the [Erlangen National High Performance Computing Center (NHR@FAU)](https://hpc.fau.de/). Conceptually, it provides tools for both instrumented (e.g. marker API) and statistical (e.g. timeline and stethoscope mode) performance monitoring.\n",
    "\n",
    "**LIKWID.jl** allows one to obtain detailed low-performance metrics for a (piece of) Julia code to answer questions such as\n",
    "* How many FLOPs have been performed?\n",
    "* What fraction of the FLOPs have been vectorized? (SIMD)\n",
    "* How much data has been read from / written to memory?\n",
    "\n",
    "**Most important commands:**\n",
    "\n",
    "* `PerfMon.supported_groups()`: List all available performance groups (\"what to measure\").\n",
    "  * Examples:\n",
    "    * \"FLOPS_SP\" / \"FLOPS_DP\": single or double precision floating point operations\n",
    "    * \"MEM\": memory related metrics\n",
    "* `@perfmon <performance_group> <code>`\n",
    "  * Example: `@perfmon \"FLOPS_DP\" myfunc(x)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information see:\n",
    "* [LIKWID.jl Documentation](https://juliaperf.github.io/LIKWID.jl/dev/)\n",
    "* [JuliaCon2022 Talk (Youtube)](https://www.youtube.com/watch?v=l2fTNfEDPC0)\n",
    "* [LIKWID Wiki](https://github.com/RRZE-HPC/likwid/wiki)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/likwid_example.png\" width=900px>"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
